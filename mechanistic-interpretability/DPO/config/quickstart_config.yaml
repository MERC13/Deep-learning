dataset:
  eval_split: test
  max_length: 256
  max_prompt_length: 128
  name: Anthropic/hh-rlhf
  train_split: train
evaluation:
  generate_samples: true
  max_samples: 50
  num_samples_to_generate: 3
lora:
  bias: none
  lora_alpha: 16
  lora_dropout: 0.05
  r: 8
  target_modules:
  - c_attn
  task_type: CAUSAL_LM
model:
  load_in_4bit: false
  load_in_8bit: false
  name: gpt2
  use_peft: true
output:
  logging_dir: outputs/logs
  output_dir: outputs/dpo_model_quickstart
seed: 42
training:
  beta: 0.1
  bf16: false
  eval_steps: 100
  fp16: true
  gradient_accumulation_steps: 2
  gradient_checkpointing: true
  learning_rate: 5.0e-05
  logging_steps: 10
  num_train_epochs: 1
  per_device_eval_batch_size: 2
  per_device_train_batch_size: 2
  save_steps: 200
  save_total_limit: 2
  warmup_steps: 50
wandb:
  enabled: false
  project: dpo-alignment
  run_name: dpo-quickstart
